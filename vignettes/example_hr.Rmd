---
title: "Classification example - HR dataset"
author: "Anna Gierlak"
date: "`r Sys.Date()`"
output: rmarkdown::html_vignette
vignette: >
  %\VignetteIndexEntry{Vignette Title}
  %\VignetteEngine{knitr::rmarkdown}
  %\VignetteEncoding{UTF-8}
---

```{r setup, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  warning = FALSE,
  message = FALSE
)
```

In this vignette we present an example of an application of `SAFE` package in case of classification problems. It is based on the `HR_data` dataset which comes from the `breakDown` package but is also available in the `SAFE` package. We will use this dataset to predict odds that someone will leave the company.

```{r}
library(SAFE)
head(HR_data)
```

## Building a black-box model

First we fit a xgboost model to the original `HR_data` dataset - this is our complex model that will serve us as a surrogate. Before that we need to prepare our data.

```{r}
data_hr_mm <- stats::model.matrix(left ~ ., data = HR_data)[,-1]
library(xgboost)
data_hr_dm <- xgb.DMatrix(data_hr_mm, label = (HR_data$left == "1"))

model_xgb <- xgb.train(params = list(objective = "binary:logistic"), data = data_hr_dm, nrounds = 100)
```

## Creating an explainer

We also create an `explainer` object that will be necessary to make use of functionalities of the `SAFE` package.

```{r}
library(DALEX)
explainer_xgb <- explain(model_xgb,
                         data = data_hr_mm,
                         y = HR_data$left,
                         label = "xgb")
explainer_xgb
```

## Creating a safe_extractor

Now, we create a `safe_extractor` object using `SAFE` package and our surrogate model. Setting the argument `verbose=FALSE` stops progress bar from printing.

```{r}
safe_extractor <- safe_extraction(explainer_xgb, verbose = FALSE)
```

Now, let's print summary for the new object we have just created.

```{r}
print(safe_extractor)
```

We can see transormation propositions for all variables in our dataset. 

In the plot below we can see which points have been chosen to be the breakpoints for a particular variable: 

```{r, fig.width=7}
plot(safe_extractor, variable = "average_montly_hours")
```

## Transforming data

Now we can use our `safe_extractor` object to create new categorical features in the given dataset.

```{r}
data_xgb <- safely_transform_data(safe_extractor, data_hr_mm, verbose = FALSE)
```

```{r, echo = FALSE}
knitr::kable(head(data_xgb))
```

We can also perform feature selection if we wish. For each original feature it keeps exactly one of their forms - original one or transformed one.

```{r, fig.width=6}
vars <- safely_select_variables(safe_extractor, data_xgb, y = HR_data$left, verbose = FALSE)
data_xgb <- cbind(HR_data["left"], data_xgb[,vars])
print(vars)
```

It can be observed that for some features the original form was preffered and for others the transformed one.

Here are the first few rows for our data after feature selection:

```{r, echo = FALSE}
knitr::kable(head(data_xgb))
```

## Creating white-box models on original and transformed datasets

Let's fit a logistic regression model as a white-box model to data containg newly created columns:

```{r}
model_lr2 <- stats::glm(left ~ ., data = data_xgb, family = binomial())
```

Moreover, we create a logistic regression model based on original `HR_data` dataset in order to check if our methodology improves results.

```{r}
model_lr1 <- stats::glm(left ~ ., data = HR_data, family = binomial())
```

## Comparing models performance

Final step is the comparison of all the models we have created. For that purpose we make predictions and print confusion matrices for all three models:

```{r}
confusion_matrix <- function(y_true, y_pred) {
  cm <- data.frame(pred_0 = c(sum(y_true==0 & y_pred==0), sum(y_true==1 & y_pred==0)),
                   pred_1 = c(sum(y_true==0 & y_pred==1), sum(y_true==1 & y_pred==1)))
  rownames(cm) <- c("actual_0", "actual_1")
  cm
}

y_true <- HR_data$left
pred_xgb <- round(predict(model_xgb, data_hr_mm))
pred_lr1 <- round(predict(model_lr1, type = 'response'))
pred_lr2 <- round(predict(model_lr2, type = 'response'))

confusion_matrix(y_true, pred_xgb)
confusion_matrix(y_true, pred_lr1)
confusion_matrix(y_true, pred_lr2)
```

We can see that the xgboost model has much higher accuracy than the original logistic regression model. But using transformations extracted from the more complicated xgboost model we can improve the performace of the simpler one.






